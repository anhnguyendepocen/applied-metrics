#+AUTHOR:      Dan Hammer
#+TITLE:       ARE213: Section 07
#+OPTIONS:     toc:nil num:nil 
#+LATEX_HEADER: \usepackage{mathrsfs}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage{booktabs}
#+LATEX_HEADER: \usepackage{dcolumn}
#+LATEX_HEADER: \usepackage{subfigure}
#+LATEX_HEADER: \usepackage[margin=1in]{geometry}
#+LATEX_HEADER: \RequirePackage{fancyvrb}
#+LATEX_HEADER: \DefineVerbatimEnvironment{verbatim}{Verbatim}{fontsize=\small,formatcom = {\color[rgb]{0.1,0.2,0.9}}}
#+LATEX: \renewcommand{\E}{\mathbb{E}}
#+LATEX: \renewcommand{\P}{\mathbb{P}}
#+LATEX: \renewcommand{\x}{{\bf x}}
#+LATEX: \renewcommand{\In}{\mathbb{I}_N}
#+LATEX: \renewcommand{\It}{\mathbb{I}_T}
#+LATEX: \renewcommand{\Int}{\mathbb{I}_{NT}}
#+LATEX: \renewcommand{\iN}{\iota}
#+LATEX: \renewcommand{\iT}{\kappa}
#+LATEX: \renewcommand{\eit}{\epsilon_{it}}
#+LATEX: \renewcommand{\lt}{\lambda_{t}}
#+LATEX: \renewcommand{\uit}{u_{it}}
#+LATEX: \renewcommand{\u}{{\bf u}}
#+LATEX: \renewcommand{\c}{{\bf c}}
#+LATEX: \renewcommand{\X}{{\bf X}}
#+LATEX: \renewcommand{\Zt}{{\bf Z}_2}
#+LATEX: \renewcommand{\Ztp}{{\bf Z}_2^{\prime}}
#+LATEX: \renewcommand{\Zo}{{\bf Z}_1}
#+LATEX: \renewcommand{\Zop}{{\bf Z}_1^{\prime}}
#+LATEX: \renewcommand{\Q}{{\bf Q}}
#+LATEX: \renewcommand{\Qp}{{\bf Q^{\prime}}}
#+LATEX: \renewcommand{\A}{{\bf A}}
#+LATEX: \renewcommand{\Xp}{{\bf X^{\prime}}}
#+LATEX: \renewcommand{\Ap}{{\bf A^{\prime}}}
#+LATEX: \renewcommand{\y}{{\bf y}}
#+LATEX: \renewcommand{\eps}{{\bf \epsilon}}
#+LATEX: \renewcommand{\with}{\hspace{8pt}\mbox{with}\hspace{6pt}}
#+LATEX: \setlength{\parindent}{0in}
#+STARTUP: fninline
#+AUTHOR: 
#+TITLE: 

*Random and Fixed Effects* \hfill
*ARE213*: Section 07 \\ \\

The presence of an unobserved, individual effect $c_i$ in a panel data
model will create a correlation over time in the outcome variable,
even if the idiosyncratic error is completely random.  Still, with the
standard assumptions in place, ordinary least squares will yield a
consistent estimator.  Consistent but inefficient.  The correlated
structure of the composite error, which imclused the individual
effect, must be made in order to correctly identify the
variance-covariance structure. \\

The following illustrates the variance of the linear estimator,
relative to the estimate that fully accounts for the fixed effects.
The data generating process is defined by $\y = 10 \c + \x + \eps$,
where $\eps \sim N(0,1)$ and $c_i \in \{1,2,3,4,5\}$.  By
construction, $\x$ and $\c$ are orthogonal, so that the pooled linear
estimator will be consistent.  Figure \ref{fig:ols} and \ref{fig:dens}
together show why the simple, pooled estimator will be inefficient.
The following code generates a panel data set with $N = 5$ and $T =
20$, and then plots the full data set with an overall linear fit in
Figure \ref{fig:ols}.  It is apparent that the linear fit is subject
to the spread of the covariate $\x$ from within a particular group.
Generally, increased variation in the cofactors will yield a more
precise estimator; but the variable intercept (read: fixed effect)
that is relegated to the error term could potentially reverse this
relationship.

#+CAPTION: Fixed effect scatterplot with pooled, linear fit
#+LABEL: fig:ols
#+begin_src R :results output graphics :file fig1.png :width 700 :height 400 :session :tangle yes :exports both 
  library(ggplot2)
  c <- rep(c(1,2,3,4,5), 20); x <- rnorm(100); eps <- rnorm(100)
  y <- 10 * c + x + eps
  p <- ggplot(data.frame(c, x, y), aes(x = x, y = y, color=c))
  (p <- p + geom_point() + geom_smooth(method=lm))
#+end_src 

#+RESULTS:
[[file:fig1.png]]

We can directly calculate the variance of the coefficient on $\x$, but
this is too much effort.  Instead, we can bootstrap the distributions,
since we really only care about the relative efficiency of the
estimators.  Figure \ref{fig:dens} clearly illustrates that the pooled
estimate is inefficient relative to directly modelling the true fixed
effect.  We use $B=100$ iterations, regenerating the data each
time. For this simple example, the value of the individual identifier
is an argument in the data generating process.  Even if this were not
the case, however, the distributions in Figure \ref{fig:dens} suggest
that there must be a better way to weight the estimator to achieve a
greater efficiency.  And in fact, there is --- clustered variance, or
the =robust= option in Stata.  The procedure to directly calculate the
clustered variance is detailed in the lecture notes, and will not be
presented again here.  

#+begin_src R :results output :exports both :tangle yes :session
  B <- 100
  fe.res <- rep(NA, B); ols.res <- rep(NA, B)

  for (i in 1:B) {
    c <- rep(c(1,2,3,4,5), 20); x <- rnorm(100); eps <- rnorm(100)
    y <- 10 * c + x + eps
  
    ols <- lm(y ~ x)
    ols.res[i] <-  ols$coefficients[["x"]]
  
    fe <- lm(y ~ x + factor(c))
    fe.res[i] <- fe$coefficients[["x"]]
  }
#+end_src 

#+RESULTS:

The geometric argument for the spread of the OLS estimates is
straightforward: variation in the covariates will have differential
impacts on the slope of the linear fit, depending on which strata is
represented.  If by chance, for example, $\x$ observations were
disproportionately selected from the upper tail of the normal
distribution when $\c == 1$, then the pooled linear fit will slope
downward.  If $\c == 5$, however, the linear fit will slope upwards.
This alone should give pause in assessing the efficiency of the pooled
estimator.  Conditional on the covariates, all observations should be
given equal weight.  The clustered variances help to mitigate this
effect by appropriately reweighting the observations.

#+CAPTION: Pooled OLS versus fixed effects, simulated distributions
#+LABEL: fig:dens
#+begin_src R :results output graphics :file fig2.png :width 700 :height 400 :session :tangle yes :exports both 
  labels <- c(rep("FE", B), rep("pooled", B)) 
  sim <- data.frame(coefficient=c(fe.res, ols.res), method=labels)
  ggplot(sim, aes(x = coefficient, fill=method)) + geom_density(alpha=0.2)
#+end_src 

Suppose that the model was not linear, but rather characterized by a
limited dependent variable.  What will happen to the consistency and
efficiency of the pooled estimate, without taking into account the
correlated structure of the error?  Figure \ref{fig:probit} indicates
that the additional variation in the composite error is not averaged
away.  Instead, the pooled OLS estimator is centered around the wrong
estimate, suggesting that the impact of $\x$ on $\y$ is smaller than
it is in truth.  The value of directly modelling the error is
complicated by the nonlinear Probit model.  The following code first
generates a binary dependent variable from the random covariates, and
then esimates the generalized linear model using the Probit function
as the binomial link.  

#+CAPTION: Limited dependent variable: inconsistency of pooled estimator
#+LABEL: fig:probit
#+begin_src R :results output graphics :file fig3.png :width 700 :height 400 :session :tangle yes :exports both 
  fe.probit <- rep(NA, B); pooled.probit <- rep(NA, B)
  
  for (i in 1:B) {
    c <- rep(c(1,2,3,4,5), 20); x <- rnorm(100); eps <- rnorm(100)
    y <- ifelse(c + x + eps > 5, 1, 0)
  
    pool <- glm(y ~ x, family = binomial(link = "probit"))
    pooled.probit[i] <- pool$coefficients[["x"]]
  
    fe <- glm(y ~ x + factor(c), family = binomial(link = "probit"))
    fe.probit[i] <- fe$coefficients[["x"]]
  }
  
  labels <- c(rep("FE", B), rep("pooled", B)) 
  sim.probit <- data.frame(coefficient=c(fe.probit, pooled.probit), method=labels)
  ggplot(sim.probit, aes(x = coefficient, fill=method)) + geom_density(alpha=0.2)
#+end_src 

#+RESULTS:
[[file:fig3.png]]

* Two-way, fixed effects panel model

Consider the fixed effects, two-way component panel data
model: $$y_{it} = \alpha + x_{it}\beta + \mu_i + \lt + \eit$$ The
fixed effects estimator of $\beta$ can be obtained by regressing $\y$
on $\X$, $\Zo$, and $\Zt$, where $\Zo = \In \otimes \iT$ is a matrix
of unit indicators and $\Zt = \iN \otimes \It$ is a matrix of time
period indicators, with $\iT$ a vector of ones of dimension $T$ and
$\iN$ a vector of ones of dimension $N$.  Note that $\dim(\Zo) = TN
\times N$ and $\dim(\Zt) = TN \times T$, assuming a balanced panel.\\

The computation for this regression is daunting, however, since it
requires the inversion of a $(k + N + T - 1) \times (k + N + T - 1)$
matrix.  The Frisch-Waugh (FW) theorem suggests that instead of a
direct regression, we can demean the variables across time and units.
The FW theorem proves that a one-way within transformation will yield
the same estimator as a fixed effects regression; and the theorem can
be extended for both individual and time effects.  The error component
structure has the form $\uit = \mu_i + \lt + \eit$, which is can be
translated into matrix form: $u = (\In \otimes \iT)\alpha + (\iN
\otimes \It)\lambda + \epsilon$, with $\alpha = [\alpha_1, \alpha_2,
\ldots \alpha_N]^{\prime}$ and $\lambda = [\lambda_1, \lambda_2,
\ldots, \lambda_T]^{\prime}$.  The error structure suggests a
candidate /purging/ matrix, which removes the individual- and
time-specific effects, along with the overall mean.  Call this matrix
$\Q = \In \otimes \It - \In \otimes \iT\iT^{\prime}/T - \It \otimes
\iN\iN^{\prime}/N + \iN\iN^{\prime}/N \otimes \iT\iT^{\prime}/T$,
which will remove, in turn, the fixed time, unit, and total (through
space and time) effect. If both $\y$ and $\X$ are sorted by unit and
time, then a regression of $\Q\y$ on $\Q\X$ should yield an unbiased
estimate of $\beta$ with a properly identified error structure.\\

Define $\P_1 = \Int - \Zo(\Zop\Zo)^{-1}\Zop$ and $\P_2 = \Int -
\Zt(\Ztp\Zt)^{-1}\Ztp$ to be the projection matrices for individual and
time fixed effects, respectively.  It is sufficient to prove that
$\P_1\P_2 = \Q$ to create a composition projection matrix: first a
within transformation ignoring the time effects followed by a within
transformation ignoring the individual effects.  First, note that:
\begin{eqnarray*} 
\P_2 = \Int - \Zt(\Ztp\Zt)^{-1}\Ztp &=& \Int - \Zt((\iN \otimes \It)^{\prime}(\iN \otimes \It) )^{-1}\Ztp\\
&=& \Int - \Zt((\iN^{\prime} \otimes \It^{\prime})(\iN \otimes \It) )^{-1}\Ztp \\
&=& \Int - \Zt(\iN^{\prime}\iN \otimes \It)^{-1}\Ztp \\
&=& \Int - \Zt(N \cdot \It)^{-1}\Ztp \\
&=& \Int - N^{-1}\Zt\Ztp \\
&=& \Int - N^{-1}(\iN \otimes \It)(\iN \otimes \It)^{\prime}\\
&=& \Int - N^{-1}(\iN\iN^{\prime} \otimes \It) \\
&=& \Int - (\iN\iN^{\prime}/N \otimes \It)
\end{eqnarray*} 

A similar, nearly symmetric argument can be made to show that $\P_1 =
\Int - (\In \otimes \iT\iT^{\prime}/T)$.  It follows that the
sequential projection using $\P_1$ and $\P_2$ is equivalent to the
two-way demeaning matrix $\Q$:
\begin{eqnarray*} 
\P_2\P_1 &=& \left(\Int - (\iN\iN^{\prime}/N \otimes \It)\right)\left(\Int - (\In \otimes \iT\iT^{\prime}/T)\right)\\
&=& \Int^2 - (\iN\iN^{\prime}/N \otimes \It) - (\In \otimes \iT\iT^{\prime}/T) + (\iN\iN^{\prime}/N \otimes \It)(\In \otimes \iT\iT^{\prime}/T) \\
&=& \Int - (\iN\iN^{\prime}/N \otimes \It) - (\In \otimes \iT\iT^{\prime}/T) + (\iN\iN^{\prime}/N \otimes \iT\iT^{\prime}/T) \\
&=& \In \otimes \It - (\iN\iN^{\prime}/N \otimes \It) - (\In \otimes \iT\iT^{\prime}/T) + (\iN\iN^{\prime}/N \otimes \iT\iT^{\prime}/T) = \Q
\end{eqnarray*} 

The sequential projection onto the fixed effect matrices is
numerically equivalent to a two-way within transformation.  The
bilinear and associative properties of the Kronecker product in the
steps above ensure that $\P_1\P_2 = \P_2\P_1 = \Q$, so that the
ordering of the within transformations make no difference.  Note that
$\Q$ is itself a projection matrix, such that $\Q$ is idempotent and
symmetric.  The estimator can be simplified: $$\beta = \left(
(\Q\X)^{\prime} \Q\X \right) ^{-1} (\Q\X)^{\prime}\Q\y = \left(
\Xp\Qp\Q\X \right)^{-1} \Xp\Qp\Q\y = \left( \Xp\Q\X \right)^{-1}
\Xp\Q\y$$

The results depend crucially on the panel being balanced.  Otherwise,
the within transformations become much, much more complicated.  The
non-uniform structure requires individual and special treatment for
each unit in the data set.  The block diagonal matrices are of various
sizes, and the dummy variable matrices must be tailored to suit the
various time intervals.  This is not to say that it cannot be done,
however, but the demeaning process becomes complicated,
circumstantial.\\

*Extra*: Some additional insight can be obtained by denoting $\A_k$ as
the demeaning matrix of dimension $k$.  Premultiplying $\X$ by $\A_n$
will return a matrix with deviations from column means; this is the
standard case that was presented in class.  If we apply the
appropriately dimensioned $\A$ matrix to the transpose of $\X$,
however, we can achieve row means.  Note that $\A_k$ is symmetric and
idempotent, so that $\A_k\A_k = \A_k$ and $\A_k^{\prime} = \A_k$.  If
we wanted to transform the matrix toward deviations from row means, we
would post-multiply by $\A_k$: $(\A_k\Xp)^{\prime} = \X\Ap_k =
\X\A_k$.  This suggests the form of $\P_1$ versus $\P_2$ above.
